{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executarea cererilor SQL cu ajutorul Dataframe\n",
    "\n",
    "### Date relaționale cu posibilitatea realizării de analize flexibile si puternice\n",
    "Store-urile de date relaționale sunt usor de construit și interogat. Utilizatorii și dezvoltatorii preferă scrierea de cereri declarative, ușor de interpretat, într-un limbaj asemanator celui natural, precum SQL.\n",
    "Pe de altă parte, pe masură ce datele cresc în volum și varietate, abordarea relațională nu scalează suficient de bine pentru a permite construirea de aplicații Big Data și sisteme analitice. Câteva dintre provocările majore sunt următoarele:\n",
    "\n",
    "* Tratarea datelor de diferite tipuri, din diferite surse, date ce pot fi structurate, semi-structurate sau nestructurate\n",
    "* Construirea pipeline-urilor ETL către și de la diferite surse de date, ce poate conduce la dezvoltare de mult cod specific (custom), mărind necesitatea intervențiilor tehnice \n",
    "* Permiterea efectuării atât de analize tradiționale, bazate pe BI (business intelligence), cât și de analize avansate (machine learning, modelare statistică etc.) - ultima dintre acestea este, cu siguranță, o provocare în sistemele relaționale  \n",
    "\n",
    "În acest domeniu al analizei Big Data, Hadoop și paradigma MapReduce au avut un succes incontestabil. Tehnologia respectivă a fost puternică, însă adeseori lentă, și le-a oferit utilizatorilor o interfață de programare procedurală ce necesită, de obicei, scriere de mult cod chiar și pentru cele mai simple transformări de date. \n",
    "Odată cu lansarea Spark, a fost revoluționat modul în care erau realizate analizele de tip Big Data, accentul fiind pus pe calculul in-memory, rezistența la defecte, abstractizări la nivel înalt și ușurința utilizării.\n",
    "\n",
    "În timp, mai multe framework-uri și sisteme precum Hive, Pig și Shark (ce a evoluat în Spark SQL) au furnizat interfețe relaționale bogate și mecanisme de interogare declarativă pentru store-urile Big Data. Provocarea care a existat a fost legată de faptul că aceste tool-uri erau fie relaționale, fie procedurale, și nu se puteau obține avantajele majore ale celor două lumi.\n",
    "\n",
    "![spark-1](https://opensource.com/sites/default/files/uploads/2_hadoop-vs-spark.png)\n",
    "\n",
    "În lumea reală, majoritatea datelor și a pipeline-urilor analitice pot implica o combinație de cod relațional și procedural. Impunerea către utilizatori să își aleagă un singur tip de cod complică lucrurile și mărește efortul utilizatorului în dezvoltarea, construirea și menținerea diferitelor aplicații și sisteme. Apache Spark SQL a fost construit peste SQL-on-Spark (denumit Shark). În loc de a forța utilizatorii să aleagă între o API relațională și una procedurală, Spark SQL permite utilizatorilor să combine cele două tipuri de API și să efectueze interogări de date, regăsiri de informație și analize care scalează la nivel de Big Data.\n",
    "\n",
    "### Spark SQL\n",
    "Spark SQL oferă o punte de legătură între cele două modele (relațional și procedural).\n",
    "Spark SQL furnizează API-ul DataFrame ce poate efectua operații relaționale atât pe date din surse externe cât și pe colecțiile distribuite, predefinite în Spark.\n",
    "\n",
    "Pentru a suporta o mare varietate de surse de date și algoritmi Big Data, Spark SQL a introdus optimizorul extensibil Catalyst, ce facilitează adăugarea surselor de date, a regulilor de optimizare și a tipurilor de date pentru analizele avansate, precum cele de machine learning.\n",
    "\n",
    "În mod esențial, Spark SQL încapsulează puterea lui Spark de a efectua calcule in-memory distribuite și robuste, la scală masivă. \n",
    "\n",
    "Spark SQL furnizează performanța SQL existentă și menține compatibilitatea cu toate structurile existente și componentele suportate de Apache Hive (un framework de warehouse popular în domeniul Big Data) ce include formate de date, funcții definite de utilizator (UDFs) și metastore. Pe lângă acestea, ajută la ingestia unui spectru larg de formate de date din surse Big Data și data warehouse-uri enterprise, formate precum JSON, Hive, Parquet etc., precum și la efectuarea unei combinații de operații relaționale și procedurale pentru analize mai complexe, avansate.\n",
    "\n",
    "![Spark-2](https://cdn-images-1.medium.com/max/2000/1*OY41hGbe4IB9-hHLRPuCHQ.png)\n",
    "\n",
    "### Spark SQL cu Dataframe API este rapid\n",
    "Spark SQL este foarte rapid, chiar și în comparație cu motoarele bazate pe C++ (precum Impala).\n",
    "\n",
    "![spark_speed](https://opensource.com/sites/default/files/uploads/9_spark-dataframes-vs-rdds-and-sql.png)\n",
    "\n",
    "Următorul grafic prezintă rezultatele unor teste asupra DataFrames vs. RDDs în diferite limbaje, rezultate ce dau o perspectivă interesantă asupra a cât de optimizate sunt DataFrame-urile.\n",
    "\n",
    "![spark-speed-2](https://opensource.com/sites/default/files/uploads/10_comparing-spark-dataframes-and-rdds.png)\n",
    "\n",
    "De ce este Spark SQL atât de rapid și optimizat? Motivul: optimizorul extensibil, **Catalyst**, bazat pe construcții de programare funcțională în Scala.\n",
    "\n",
    "Design-ul extensibil al lui Catalyst are 2 scopuri:\n",
    "\n",
    "* Facilitează adăugarea de noi tehnici și functionalități de optimizare în Spark SQL, în special pentru a trata diferite probleme referitoare la Big Data, date semi-structurate și analize avansate.\n",
    "* Ușurința de a extinde optimizorul - de exemplu, adăugând reguli specifice surselor de date ce pot \"împinge\" filtrările sau agregările în sisteme de stocare externe sau pot oferi suport pentru noi tipuri de date \n",
    "\n",
    "Catalyst suportă atât optimizarea bazată pe reguli, cât și pe cea bazată pe cost. Anterior lui, în trecut au mai fost propuși optimizori extensibili, însă aceia necesitau un domain-specific language complex pentru a specifica regulile. De obicei, aceasta însemna un timp semnificativ de învățare și, ulterior, de mentenanță. Spre deosebire de acei optimizori, Catalyst folosește caracteristicile standard ale limbajului Scala, precum pattern-matching, permițând dezvoltatorilor să utilizeze un limbaj cunoscut pentru a specifica regulile cu usurință.\n",
    "\n",
    "\n",
    "![catalyst-2](https://cdn-images-1.medium.com/max/1500/1*81ZOMxCci-tM2b-HNUX6Ww.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referinte utile pentru acest Notebook\n",
    "* [PySpark in Jupyter Notebook — Working with Dataframe & JDBC Data Sources](https://medium.com/@thucnc/pyspark-in-jupyter-notebook-working-with-dataframe-jdbc-data-sources-6f3d39300bf6)\n",
    "* [PySpark - Working with JDBC Sqlite database](http://mitzen.blogspot.com/2017/06/pyspark-working-with-jdbc-sqlite.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crearea unui obiect SparkSession și citirea setului de date CSV referitor la prețurile acțiunilor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext as sc\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark1 = SparkSession.builder.appName('SQL').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark1.read.csv('../../Data/appl_stock.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creați o vizualizare temporară corespunzătoare acestui DataFrame, denumită `stock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executați o cerere SQL direct pe acest view. Ce returnează (ca tip de date)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark1.sql(\"SELECT * FROM stock LIMIT 5\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Afișați coloanele rezultatului\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Afișați rezultatul\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executați cereri mai complexe\n",
    "Câte valori ale câmpului `Close` sunt > 500?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_greater_500 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Care este media valorilor `Open` a înregistrărilor pentru care `Volume` este > 120 milioane sau < 110 milioane?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg_1 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citiți un fișier (și creați un DataFrame) executând direct o metodă `spark.sql` pe fișier\n",
    "Observați sintaxa `csv.<path->filename.csv>` din SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = spark1.sql(\"SELECT * FROM csv.`../../Data/sales_info.csv`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citiți tabelele dintr-o bază de date SQLLite locală folosind o conexiune `JDBC`\n",
    "Folosim baza de date chinook din tutorialul SQLite. [Fișierul se poate obține de aici](http://www.sqlitetutorial.net/sqlite-sample-database/). Dezarhivați fisierul în folder-ul Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executați comanda `cd` pentru a ajunge la folderul unde se află fișierele jar ale lui PySpark. Apoi descărcați jar-ul SQLLite de la [given URL](https://mvnrepository.com/artifact/org.xerial/sqlite-jdbc) SAU (în Windows) descărcați și copiați fișierul jar direct în folder-ul respectiv (în Anaconda: <user_home>\\anaconda3\\Lib\\site-packages\\pyspark\\jars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd <cale_catre_jars>\n",
    "!curl https://repo1.maven.org/maven2/org/xerial/sqlite-jdbc/3.28.0/sqlite-jdbc-3.28.0.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definiți valorile pentru driver, calea către fișierul local .db și adăugați acea cale la string-ul `jdbc:sqlite` pentru a forma `url`-ul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = \"org.sqlite.JDBC\"\n",
    "path = \"../../Data/chinook.db\"\n",
    "url = \"jdbc:sqlite:\" + path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definiți numele tabelului ce va fi citit (`tablename` )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename = \"albums\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_albums = spark1.read.format(\"jdbc\").option(\"url\", url).option(\"dbtable\", tablename).option(\"driver\", driver).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_albums.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Afișați schema datelor\n",
    "df_albums.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creați o vizualizare temporară, denumită `albums`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_albums.createTempView('albums')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Citiți tabelul `artists`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_artists = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Afișați df_artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creați o vizualizare temporară, denumită `artists`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testați o cerere SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark1.sql(\"SELECT * FROM artists WHERE length(Name)<10 LIMIT 10\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efectuați operația de join a vizualizărilor `albums` și `artists` ce va avea ca rezultat un singur DataFrame, cu ajutorul unei cereri SQL. Rezultatul va include și artiștii care nu au albume asociate și va fi ordonat după coloana `ArtistId`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_combined = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Care este diferența dintre o vizualizare SQL temporară și una globală?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O vizualizare temporară nu persistă (nefiind disponibilă în cadrul mai multor sesiuni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artists.createOrReplaceTempView(\"temp_artists\")\n",
    "\n",
    "df_temp = spark1.sql(\"SELECT * FROM temp_artists LIMIT 10\")\n",
    "df_temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se creează o nouă sesiune, dar vizualizarea temporară `temp_artists` nu poate fi accesată"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark2 = spark1.newSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = spark2.sql(\"SELECT * FROM temp_artists LIMIT 10\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se creează o vizualizare globală în această sesiune\n",
    "Vizualizarea globală temporară este legată de o bază de date menținută de sistem `global_temp`. Deci, vizualizarea va fi referită ca atare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename = \"artists\"\n",
    "df_artists = spark2.read.format(\"jdbc\").option(\"url\", url).option(\"dbtable\", tablename).option(\"driver\", driver).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artists.createOrReplaceGlobalTempView(\"global_artists\")\n",
    "\n",
    "df_global = spark2.sql(\"SELECT * FROM global_temp.global_artists LIMIT 10\")\n",
    "df_global.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Porniți o nouă sesiune. Vizualizarea `global_artists` va putea fi accesată în aceasta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark3 = spark1.newSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global = spark3.sql(\"SELECT * FROM global_temp.global_artists LIMIT 10\")\n",
    "df_global.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

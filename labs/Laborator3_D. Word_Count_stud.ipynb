{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numărarea cuvintelor\n",
    "\n",
    "### Numărarea aparițiilor cuvintelor într-un text este, de obicei, primul exercițiu pentru aplicarea metodei map-reduce.\n",
    "\n",
    "## Problema\n",
    "**Input:** Un fișier text ce constă din cuvinte separate prin spații.  \n",
    "**Output:** O listă de cuvinte și numărul lor de apariții. Lista va fi sortată descrescător după numărul de apariții.\n",
    "\n",
    "Vom folosi fișierul ce conține cartea \"Moby Dick\" ca input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Porniti SparkContext cu 4 core-uri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiți o funcție pentru afișarea corespunzătoare a planului de execuție"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_plan(rdd):\n",
    "    for x in rdd.toDebugString().decode().split('\\n'):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizați metoda `textFile()` pentru a citi textul din fișier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "text_file = sc.textFile(\"../Data/Moby-Dick.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pașii pentru numărarea cuvintelor\n",
    "\n",
    "* se separă elementele unei linii în funcție de separatori (spații).\n",
    "* se mapează `word` la `(word,1)`\n",
    "* se numără aparițiile fiecărui cuvânt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "words =     text_file.flatMap(lambda line: line.split(\" \"))\n",
    "#obtineti cuvintele nevide\n",
    "# not_empty = \n",
    "#mapati word -> (word, 1)\n",
    "#key_values= \n",
    "#numarati cuvintele, cu ajutorul reduceByKey, cu o functie lambda ca argument\n",
    "#counts=    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap()\n",
    "In linia de cod:\n",
    "```python\n",
    "words =     text_file.flatMap(lambda line: line.split(\" \"))\n",
    "```\n",
    "este utilizat `flatMap` în loc de `map`.\n",
    "\n",
    "Motivul este acela că operația `line.split(\" \")` generează o **listă** de string-uri, deci dacă am folosi `map` rezultatul ar fi un RDD de liste de cuvinte, nu un RDD de cuvinte.\n",
    "\n",
    "Diferența dintre `map` și `flatMap` este aceea că `flatMap` asteaptă câte o listă care provine din rezultatul câte unui map și **concatenează** listele spre a forma RDD-ul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planul de execuție\n",
    "În ultima celulă am definit planul de execuție, dar nu l-am executat.\n",
    "\n",
    "* Pregătirea planului a durat ~100ms, un interval de timp netrivial, dar mai scurt decat timpul necesar execuției.\n",
    "* Vom analiza planul de execuție."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detaliile planului de execuție\n",
    "Pentru a determina cărui RDD îi corespunde fiecare pas din plan, vom afișa planul de execuție pentru fiecare dintre RDD-uri.  \n",
    "\n",
    "Se va observa că planurile de execuție pentru `words`, `not_empty` și `key_values` sunt aceleași."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_plan(text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_plan(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_plan(not_empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_plan(key_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_plan(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Execution plan   | RDD |  Comments |\n",
    "| :---------------------------------------------------------------- | :------------: | :--- |\n",
    "|`(2)_PythonRDD[6] at RDD at PythonRDD.scala:48 []`| **counts** | Final RDD|\n",
    "|`_/__MapPartitionsRDD[5] at mapPartitions at PythonRDD.scala:436 []`| **---\"---** |\n",
    "|`_/__ShuffledRDD[4] at partitionBy at NativeMethodAccessorImpl.java:0 [`| **---\"---** | RDD is partitioned by key |\n",
    "|`_+-(2)_PairwiseRDD[3] at reduceByKey at <timed exec>:4 []`| **---\"---** | Perform mapByKey |\n",
    "|`____/__PythonRDD[2] at reduceByKey at <timed exec>:4 []`| **words, not_empty, key_values** | The result of  partitioning into words|\n",
    "| | |  removing empties, and making into (word,1) pairs|\n",
    "|`____/__../../Data/Moby-Dick.txt MapPartitionsRDD[1] at textFile at Nat`| **text_file** | The partitioned text |\n",
    "|`____/__../../Data/Moby-Dick.txt HadoopRDD[0] at textFile at NativeMeth`| **---\"---** | The text source |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execuția\n",
    "Numărăm aparițiile fiecărui cuvânt. Abia acum modelul cu execuție _lazy_ efectuează operațiile, motiv pentru care execuția durează mai mult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Run #1\n",
    "Count=counts.count()  # Count = the number of different words\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y) # \n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amortizarea\n",
    "Atunci când aceleași comenzi sunt efectuate în mod repetat pe aceleași date, timpul pentru execuțiile ulterioare tinde să descrească.\n",
    "\n",
    "Celulele de mai jos sunt identice celei anterioare, cu o excepție la `Run #3`\n",
    "\n",
    "Observăm că `Run #2` durează mai puțin decât `Run #1` cu toate că nu a fost aplicată explicit metoda `cache()`. Motivul este acela că Spark pune în cache (materializează) `key_values`, înainte de a executa `reduceByKey()`. Efectuarea operației _reduceByKey_ necesită o amestecare (_shuffle_), iar un _shuffle_ necesită materializarea RDD-ului de intrare. Prin urmare, observăm ca punerea în cache (caching-ul) are uneori loc chiar dacă nu a fost solicitat explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Run #2\n",
    "Count=counts.count()\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching Explicit\n",
    "În `Run #3` solicităm explicit punerea în _cache_ a lui `counts`. Aceasta va reduce timpul la următoarea execuție `Run #4`, însă nu în mod semnificativ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Run #3, cache\n",
    "Count=counts.cache().count()\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Run #4\n",
    "Count=counts.count()\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Run #5\n",
    "Count=counts.count()\n",
    "Sum=counts.map(lambda x:x[1]).reduce(lambda x,y:x+y)\n",
    "print('Different words=%5.0f, total words=%6.0f, mean no. occurances per word=%4.2f'%(Count,Sum,float(Sum)/Count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
